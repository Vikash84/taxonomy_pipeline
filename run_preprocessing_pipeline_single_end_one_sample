#!/bin/bash

#
# This script runs a "pre-processing" analysis pipeline on a single-end sequencing dataset.  

# What it does is:
#
# (1) uses cutadapt to trim low quality sequences, low quality bases, and adapter sequences
#     You may need to modify the adapter sequence file (see below)
#
#     outputs  <file_base>_R1_f.fastq 
#
# (2) uses cd-hit to collapse duplicate reads. 
#
#     outputs  <file_base>_R1_fu.fastq 
# 
#
# Input:
#
# This script takes as an argument a file base name.  
# It expects a file of the form: <base_name>_R1.fastq to exist
#
# Dependencies:
#
# - cutadapt
# - cd-hit-dup
#
# Mark Stenglein
#
# 10/6/2015 
#

file_base=$1

log=${file_base}.pipeline.log

#
# Hardcoded paths --> may need to change
#


# bracket to redirect all stdout output to log file
{

   echo "***********************************" 
   echo "begin processing sample: $file_base" 
   date 

   f1=${file_base}_R1.fastq
   f2=${file_base}_R2.fastq

   # use cutadapt to trim adapter sequences and do quality trimming and throw out too-short reads

   cutadapt -a AGATCGGAAGAGC -g GCTCTTCCGATCT -a AGATGTGTATAAGAGACAG -g CTGTCTCTTATACACATCT -q 30,30 --minimum-length 80 -u 1 -o ${file_base}_R1_f.fastq $f1 
	# parts of this command:

   # cutadapt \ 
	# -a AGATCGGAAGAGC -A AGATCGGAAGAGC -g GCTCTTCCGATCT -G GCTCTTCCGATCT  \                           # TruSeq style adapters
   # -a AGATGTGTATAAGAGACAG -A AGATGTGTATAAGAGACAG -g CTGTCTCTTATACACATCT -G CTGTCTCTTATACACATCT \    # Nextera adapters
	# -q 30,30 \                                                                                       # filter low qual seqs -> see cutadapt documentation
	# --minimum-length 80 \                                                                            # ditch seqs shorter than this and their pairs
	# -u 1  \                                                                                          # trim the last (1 3') base 
	# -o ${file_base}_R1_f.fastq  \                                                                    # trimmed (R1) output
	# $f1                                                                                              # the name of the input files  


	# name of cutadapt output
   f1=${file_base}_R1_f.fastq

   # collapse to "unique" reads based using cd-hit-dup
	# i.e. remove likely PCR duplicates
   # allow up to 2 mismatches in the first 50 bases (-e 2 -u 50): 
   cd-hit-dup -i $f1 -o ${file_base}_R1_fu.fastq -e 2 -u 50

} 2>&1  | tee -a $log  # output tee'd to a logfile



